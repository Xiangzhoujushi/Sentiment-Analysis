<<<<<<< HEAD
README for project

For lab4-part2.ipynb, it is the file for option 2
(For running the interactive python program, the user just need to download Jupiter notebook first, and then type "jupyter notebook" in the command window to start the local server for running the jupyter notebook. For each input and output, just click to see the result and you will get a final result

While the lab4.py is the file for option 3. 

I choose three classifiers and compare them, (1) SGD, stochastic gradient descent, (2) SVM, support vector machine (3) neuronetwork. All of them are good supervised classifiers and I test the data and measure the accuracy rate for evaluations. 
For SGD, it is a good linear classifier which can predict the value of input in all kinds of scenarios because of its stochastic feature. SVM is a good classifier which uses second 
Order optimization to find the linear classifier. Neuronetwork is a deep learning method which deals with multi-layer scenarios, and is not linear. I use all of them and want to compare the accuracy between different common classifiers. 

From my tests based on different parameter settings, the SGD gives the lowest accuracy rate, while the neuronetwork method gives the highest accuracy rate, in case that the word vector extraction does not directly reflect the emotion expressed from each word, but it sometimes is encoded by the frequency of the words, which is not that related to the mood of the sentence. In such cases, I only got accuracy rte around 40% for all those 3 methods at most. 

=======
# Group
Peiyuan Tang tang.794
Patrick Green green.1125
Trevor Rambacher rambacher.8

# Lab 4: Sentiment Analysis
  DUE : 4/17/2018
  Option 3: Deep Learning / Option 2: Classifier Compare


# Overview:
We initially intended to complete Option 3 (Deep Learning), but we were unable to finish.
Option 2 (Classifier Comparison) was completed without a writeup by one of the group members to compensate.

NOTE: If only considering one part for grading, please grade lab4-option3, even though it is incomplete.

### Option 3 ###

# Initialization / word2vec
We used Google's pretraineed word2vec vectors for training.
A copy of the file must be loaded locally for the code to work: https://code.google.com/archive/p/word2vec/
The path should be specified in the initialization
This is a large file that may not load on all machines (there are restrictions on 64-bit python and memory)
REF: http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/

    word2vec_path = 'D:/Temp/GoogleNews-vectors-negative300.bin'
    from lab4 import *; sdl = SentimentDeepLearner(word2vec_path=word2vec_path)

# Preprocessing
Generate the "labeled_sentences.txt" file
This preprocessing performs several steps:
- Condenses data across multiple files to a single file
- Represents sentences as pipe-separated word parse
- Standardizes sentence length to maximum length (56 words) and pads others with empty string
- Converts the continuous sentiment attribute to label bins: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
- Strips out the "dev" parition (train and test only)
This file is then used directly for training

    sdl._build_labeled_sentences()

# Training
Specify the batch size, iterations and number of lstm
This code is incomplete and does not run.
INCOMPLETE:
- Pull labels in batching for classification evaluation
- Randomize batches instead of continuous blocks
- Correctly attach lstm and classification layers

    batch_size=24
    iterations=10000
    lstm_count=64
    sdl.train(batch_size=batch_size, iterations=iterations, lstm_count=lstm_count)
>>>>>>> e10b0704da2ac4f49fb7cf224e501c5a1b8e65fe
